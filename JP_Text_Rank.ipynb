{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JP_Text_Rank",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "izHMWeZDFm-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06fa21b-65e1-4718-9347-f696d2a9c05e"
      },
      "source": [
        "!pip install unidic-lite\n",
        "!pip install --no-binary :all: mecab-python3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidic-lite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/d2/a4233f65f718f27065a4cf23a2c4f05d8bd4c75821e092060c4efaf28e66/unidic-lite-1.0.7.tar.gz (47.3MB)\n",
            "\u001b[K     |████████████████████████████████| 47.3MB 104kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.7-cp36-none-any.whl size=47556593 sha256=e9d96d0a0990ccbd4dc29ef90f754077775dca419be95df1fd68a42f6d02d301\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/82/7d/086724645e33a575aafd0b1dae2835c37d2c00c6a0a96ee3a0\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.7\n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (1.0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwTf3bRCNagi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7985ce7f-1d01-4ecc-872a-d8af43d0de01"
      },
      "source": [
        "!pip install MeCab-python3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting MeCab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/f0/b57bfb29abd6b898d7137f4a276a338d2565f28a2098d60714388d119f3e/mecab_python3-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (487kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 10.1MB/s \n",
            "\u001b[?25hInstalling collected packages: MeCab-python3\n",
            "Successfully installed MeCab-python3-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGuvwFMvBpaK"
      },
      "source": [
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1MZNhCBNKd0"
      },
      "source": [
        "import MeCab\n",
        "import math\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "from itertools import product, count\n",
        "from gensim.models import word2vec\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9RbFhXqPWJ0"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "model_path = '/content/word2vec.gensim.model'\n",
        "model = Word2Vec.load(model_path)\n",
        "np.seterr(all='warn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-90d76Eagw-"
      },
      "source": [
        "def cut_sentences(sentence):\n",
        "  puns = frozenset(u'.!?')\n",
        "  tmp = []\n",
        "  for jp in sentence:\n",
        "    tmp.append(jp)\n",
        "    if puns.__contains__(jp):\n",
        "      yield ''.join(tmp)\n",
        "      tmp = []\n",
        "  yield ''.join(tmp)\n",
        "\n",
        "#if need stop words \n",
        "## for news i dont think need clear stop words\n",
        "def create_stopwords():\n",
        "    stop_list = [line.strip() for line in open(\"stopwords.txt\", 'r', encoding='utf-8').readlines()]\n",
        "    return stop_list\n",
        "\n",
        "\n",
        "\n",
        "def two_sentences_similarity(sents_1,sents_2):\n",
        "  counter = 0\n",
        "  for sent in sents_1:\n",
        "    if sent in sents_2:\n",
        "      counter +=  1\n",
        "  return counter/(math.log(len(sents_1) + len(sents_2)))\n",
        "\n",
        "def create_graph(word_sent):\n",
        "  num = len(word_sent)\n",
        "  board = [[0.0 for _ in range(num)] for _ in range(num)]\n",
        "  for i, j in product(range(num), repeat=2):\n",
        "      if i != j:\n",
        "          board[i][j] = compute_similarity_by_avg(word_sent[i], word_sent[j])\n",
        "  return board\n",
        "\n",
        "def cosine_similarity(vec1,vec2):\n",
        "  tx = np.array(vec1)\n",
        "  ty = np.array(vec2)\n",
        "  cos1 = np.sum(tx * ty)\n",
        "  cos21 = np.sqrt(tx ** 2)\n",
        "  cos22 = np.sqrt(ty ** 2)\n",
        "  f2 = cos21 * cos22\n",
        "  #print(\"cos1\"+cos1)\n",
        "  #print(\"/\"+f2)\n",
        "  cosine_value = cos1 /f2\n",
        "  return cosine_value\n",
        "\n",
        "def compute_similarity_by_avg(sents_1,sents_2):\n",
        "#对两个句子求平均词向量\n",
        "    if len(sents_1) == 0 or len(sents_2) == 0:\n",
        "        return 0.0\n",
        "    if sents_1[0] in model:\n",
        "      vec1 = model[sents_1[0]]\n",
        "    else:\n",
        "      return 0.0\n",
        "    for word1 in sents_1[1:]:\n",
        "      if word1 in model:\n",
        "         vec1 = vec1 + model[word1]\n",
        "      else:\n",
        "         vec1 = vec1 + 0.0\n",
        "      \n",
        "      if sents_2[0] in model:  \n",
        "        vec2 = model[sents_2[0]]\n",
        "      \n",
        "      else:\n",
        "        vec2= 0.0\n",
        "      \n",
        "      for word2 in sents_2[1:]:\n",
        "        if word2 in model:\n",
        "          vec2 = vec2 + model[word2]\n",
        "      else:\n",
        "          vec2 = vec2 + 0.0\n",
        "\n",
        "    similarity = cosine_similarity(vec1 / len(sents_1), vec2 / len(sents_2))\n",
        "    return similarity  \n",
        "\n",
        "def calculate_score(weight_graph, scores, i):\n",
        "    \"\"\"\n",
        "    计算句子在图中的分数\n",
        "    :param weight_graph:\n",
        "    :param scores:\n",
        "    :param i:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    length = len(weight_graph)\n",
        "    d = 0.85\n",
        "    added_score = 0.0\n",
        " \n",
        "    for j in range(length):\n",
        "        fraction = 0.0\n",
        "        denominator = 0.0\n",
        "        # 计算分子\n",
        "        fraction = weight_graph[j][i] * scores[j]\n",
        "        # 计算分母\n",
        "        for k in range(length):\n",
        "            denominator += weight_graph[j][k]\n",
        "            #if denominator == 0:\n",
        "             # denominator = 1\n",
        "        added_score += fraction / denominator\n",
        "    # 算出最终的分数\n",
        "    weighted_score = (1 - d) + d * added_score\n",
        "    return weighted_score  \n",
        "\n",
        "def weight_sentences_rank(weight_graph):\n",
        "    '''\n",
        "    输入相似度的图（矩阵)\n",
        "    返回各个句子的分数\n",
        "    :param weight_graph:\n",
        "    :return:\n",
        "    '''\n",
        "    # 初始分数设置为0.5\n",
        "    scores = [0.5 for _ in range(len(weight_graph))]\n",
        "    old_scores = [0.0 for _ in range(len(weight_graph))]\n",
        " \n",
        "    # 开始迭代\n",
        "    while different(scores, old_scores):\n",
        "        for i in range(len(weight_graph)):\n",
        "            old_scores[i] = scores[i]\n",
        "        for i in range(len(weight_graph)):\n",
        "            scores[i] = calculate_score(weight_graph, scores, i)\n",
        "            scores[i] = np.mean(scores[i])\n",
        "            #print(scores[i])\n",
        "    return scores\n",
        "\n",
        "def different(scores, old_scores):\n",
        "    '''\n",
        "    判断前后分数有无变化\n",
        "    :param scores:\n",
        "    :param old_scores:\n",
        "    :return:\n",
        "    '''\n",
        "    flag = False\n",
        "    for i in range(len(scores)):\n",
        "        s = scores[i] - old_scores[i]\n",
        "        if math.fabs(s) >= 0.0001:\n",
        "            flag = True\n",
        "            break\n",
        "    return flag\n",
        "\n",
        "def filter_symbols(sents):\n",
        "    stopwords = create_stopwords() + ['。', ' ', '.']\n",
        "    _sents = []\n",
        "    for sentence in sents:\n",
        "        for word in sentence:\n",
        "            if word in stopwords:\n",
        "                sentence.remove(word)\n",
        "        if sentence:\n",
        "            _sents.append(sentence)\n",
        "    return _sents\n",
        "\n",
        "def filter_symbols(sents):\n",
        "  #预处理\n",
        "    stopwords = create_stopwords() + ['。', ' ', '.']\n",
        "    _sents = []\n",
        "    for sentence in sents:\n",
        "        for word in sentence:\n",
        "            if word in stopwords:\n",
        "                sentence.remove(word)#2\n",
        "        if sentence:\n",
        "            _sents.append(sentence)\n",
        "    return _sents\n",
        "\n",
        "def filter_model(sents):\n",
        "    _sents = []\n",
        "    for sentence in sents:\n",
        "      if sentence:\n",
        "        _sents.append(sentence)\n",
        "    return _sents\n",
        "\n",
        "def summarize(text,n):\n",
        "  tokens = cut_sentences(text)\n",
        "  #tokens = \"2020年の主要16品目の半導体市場規模は、新型コロナウイルス感染症の影響による外出制限に伴うリモートワークの推進などを追い風に、半導体デバイスの需要が増加した結果、前年比14.4％増の26兆678億円が見込まれるという。その後も米中貿易摩擦や日韓貿易摩擦による半導体市場への影響が見受けられるものの、リモートワークの一般化、AIの普及などに伴い、半導体の活用が進むとみられ、2025年にはその市場規模は2019年比88.9％増の43兆470億円に到達するものと同社は予測している。品目別にみると、DRAMの市場規模がもっとも大きく、次いでNAND、モバイル機器用APと続く。DRAMやNANDは、データセンターの投資が好調であることから継続的に伸長していくとみられる。スマートフォン(スマホ)やスマートグラスなどに搭載されるモバイル機器用APは、5G通信のサービス開始に伴い5G対応スマートフォンの需要が高まっていることや、スマートウォッチやスマートグラスなどへ用途が広がっていることから、2021年以降市場は堅調に拡大していくと予想されるという。もっとも大きな市場として期待されるDRAMの2020年市場について同社は、前年比19.4％増の8兆円と見込んでいる。CPUの開発遅延などにより投資が緩やかであったデータセンターにおいて、リモートワークの普及によるデータ通信料の増加や代替CPUの登場によって投資が活発化しており、今後も投資が継続していくことが期待されるため、2025年には2019年比94.0％増の13兆円に達すると予測している。メモリ分野のもう1つの大市場であるNANDの2020年市場規模は前年比34.9％増の6兆8000億円が見込まれると富士キメラでは見込んでいる。また、今後もデータセンターを中心にストレージに対する投資が進むことから、需要は堅調に推移し、2025年には2019年比で3.1倍となる15兆5000億円に達すると予測している。半導体業界が期待してきた自動車分野の中核の1つ、自動車用SoCの2020年市場は、新型コロナの影響で自動車販売が不振に陥っていることから前年比12.2％減の2030億円となると見込まれているが、今後は自動運転レベルの向上に伴い、1台当たりの搭載個数は増加していくこととなり、需要の増加が期待されることから、2025年には2019年比2.0倍の4659億円に達すると予測している。スマートフォンへの搭載数拡大のほか、自動車での活用も進むイメージセンサ市場について同社は、2020年が前年比3.0％増の1兆9680億円と見込んでいる。現在、スマートフォンへの搭載数が増加しているが、今後もそのトレンドは継続するほか、自動車でも搭載数が増加していくと予測されるため、市場の拡大が続くことが期待され、2025年には2019年比38.5％増の2兆6460億円に達すると予測している。そしてセンサ系としては、ToFセンサについて同社は、5Gや画像認識技術の進化に伴い、ARの普及が進むことが期待されており、2020年の市場規模は前年比6.4％増の1055億円と見込んでいる。また、スマホへの搭載は当然ながら、中長期的にはスマートグラスへの搭載も進むことが期待されているため、2025年には2019年比3.2倍増の3143億円と予測している。\"\n",
        "  sentences = []\n",
        "  sents = []\n",
        "  mecabTagger = MeCab.Tagger(\"-Owakati\")\n",
        "  for sent in tokens:\n",
        "    sentences.append(sent)\n",
        "    #print(\"sent\"+sent)\n",
        "    #print(mecabTagger.parse(sent))\n",
        "    sents.append(mecabTagger.parse(sent))\n",
        "    #print(\"sent1\"+sents)\n",
        "  sents = filter_model(sents)\n",
        "  graph = create_graph(sents)\n",
        "\n",
        "  scores = weight_sentences_rank(graph)\n",
        "  sent_selected = nlargest(n, zip(scores, count()))\n",
        "  sent_index = []\n",
        "  for i in range(n):\n",
        "      sent_index.append(sent_selected[i][1])#bug\n",
        "  return [sentences[i] for i in sent_index] #bug cosine_similarity test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM1oHefLA6bK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "c567f786-b8e1-4efc-c029-887e1bab5149"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "m=MeCab.Tagger(\"-Owakati\")\n",
        "node = m.parse(\"富士通と東京品川病院は9月2日、新型コロナウイルス肺炎の診断に有効とされる胸部CT(Computed Tomography:コンピューター断層撮影)検査による画像診断の支援を行うAI技術の共同研究開発を同日から実施すると明らかにした。新型コロナウイルス感染の疑いが強い患者の治療を行う際、PCR検査結果だけでなく、血液検査や胸部CT検査など、そのほかの検査結果を踏まえて総合的に診断および治療方針の決定を行う。PCR検査が陰性でも、そのほかの検査で新型コロナウイルス肺炎と診断される場合もあり、胸部CT検査による画像診断は重要な位置づけとなっているという。肺疾患の診断では、医師が患者の胸部CT画像に映った病変部の陰影の特徴に基づき診断しているが、異常な陰影の判別だけでなく、肺全体にわたる陰影の立体的な分布状況を把握するため、患者一人あたり数百枚にもおよぶ胸部CT画像を目視で確認している。特に、新型コロナウイルス感染症拡大に伴い、医師の負担を軽減し、スピーディーな診断を支援する技術が求められているほか、問診で新型コロナウイルス感染の可能性が低く、PCR検査が実施されない場合、その患者の胸部CT画像から新型コロナウイルス肺炎を見つけ出すことも重要だという。これらの診断支援の必要性から両者は共同で、新型コロナウイルス肺炎の診断に有効な手法とされる胸部CT検査に対して、AIを活用した医師の画像診断支援技術の開発を開始。具体的には、東京品川病院が有する過去の新型コロナウイルス肺炎の胸部CT画像データから肺の異常陰影パターンを検出して、それらのデータをAIに学習させることで新型コロナウイルス肺炎の可能性を示すAI技術を開発し、その技術の有効性を両者で検証する。医師が新型コロナウイルス肺炎を診断する際、肺に見られる異常陰影のパターンと肺全体の異常陰影の広がり方が診断のための重要な情報となっており、異常陰影のパターンの検出は富士通研究所が開発したAIを活用して行い、CT画像上で肺を右肺末梢、右肺中枢、左肺中枢、左肺末梢の4つの領域に分割し、各領域の上下方向の陰影分布をヒストグラム化(データの分布状況を視覚的に認識可能な統計グラフの1つ)する。これにより、三次元的な陰影の広がりを数値化し、検出された異常陰影パターンと陰影分布を用いて新型コロナウイルス肺炎を判別するAIを新たに開発。AIで新型コロナウイルス肺炎の可能性を示すことで、医師が\")\n",
        "node = mt.parse(\"昨日、お笑い番組を見た。\")\n",
        "#print(node)\n",
        "\n",
        "for word in node.split():\n",
        "  #print(word)\n",
        "  print(m[word])\n",
        "  print(m[word].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-7833a1aea523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"昨日、お笑い番組を見た。\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(node)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"昨日\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#print(word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Tagger' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai22FdTzHp_1"
      },
      "source": [
        "#model = pickle.load('lstm_datamodel.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwcswuytwOos"
      },
      "source": [
        "with open(\"/content/news.txt\", \"r\", encoding='utf-8') as myfile:\n",
        "        text = myfile.read().replace('\\n', '')\n",
        "        summarys=summarize(\"2020年の主要16品目の半導体市場規模は、新型コロナウイルス感染症の影響による外出制限に伴うリモートワークの推進などを追い風に、半導体デバイスの需要が増加した結果、前年比14.4％増の26兆678億円が見込まれるという。その後も米中貿易摩擦や日韓貿易摩擦による半導体市場への影響が見受けられるものの、リモートワークの一般化、AIの普及などに伴い、半導体の活用が進むとみられ、2025年にはその市場規模は2019年比88.9％増の43兆470億円に到達するものと同社は予測している。品目別にみると、DRAMの市場規模がもっとも大きく、次いでNAND、モバイル機器用APと続く。DRAMやNANDは、データセンターの投資が好調であることから継続的に伸長していくとみられる。スマートフォン(スマホ)やスマートグラスなどに搭載されるモバイル機器用APは、5G通信のサービス開始に伴い5G対応スマートフォンの需要が高まっていることや、スマートウォッチやスマートグラスなどへ用途が広がっていることから、2021年以降市場は堅調に拡大していくと予想されるという。もっとも大きな市場として期待されるDRAMの2020年市場について同社は、前年比19.4％増の8兆円と見込んでいる。CPUの開発遅延などにより投資が緩やかであったデータセンターにおいて、リモートワークの普及によるデータ通信料の増加や代替CPUの登場によって投資が活発化しており、今後も投資が継続していくことが期待されるため、2025年には2019年比94.0％増の13兆円に達すると予測している。メモリ分野のもう1つの大市場であるNANDの2020年市場規模は前年比34.9％増の6兆8000億円が見込まれると富士キメラでは見込んでいる。また、今後もデータセンターを中心にストレージに対する投資が進むことから、需要は堅調に推移し、2025年には2019年比で3.1倍となる15兆5000億円に達すると予測している。半導体業界が期待してきた自動車分野の中核の1つ、自動車用SoCの2020年市場は、新型コロナの影響で自動車販売が不振に陥っていることから前年比12.2％減の2030億円となると見込まれているが、今後は自動運転レベルの向上に伴い、1台当たりの搭載個数は増加していくこととなり、需要の増加が期待されることから、2025年には2019年比2.0倍の4659億円に達すると予測している。スマートフォンへの搭載数拡大のほか、自動車での活用も進むイメージセンサ市場について同社は、2020年が前年比3.0％増の1兆9680億円と見込んでいる。現在、スマートフォンへの搭載数が増加しているが、今後もそのトレンドは継続するほか、自動車でも搭載数が増加していくと予測されるため、市場の拡大が続くことが期待され、2025年には2019年比38.5％増の2兆6460億円に達すると予測している。そしてセンサ系としては、ToFセンサについて同社は、5Gや画像認識技術の進化に伴い、ARの普及が進むことが期待されており、2020年の市場規模は前年比6.4％増の1055億円と見込んでいる。また、スマホへの搭載は当然ながら、中長期的にはスマートグラスへの搭載も進むことが期待されているため、2025年には2019年比3.2倍増の3143億円と予測している。\",1)\n",
        "        print(summarys)\n",
        "        for each in summarys:\n",
        "            print (each+\"\\n\"+\"*\"*10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjg8Fh1Dv4eF"
      },
      "source": [
        "ここまでは自分でTEXTRANKアルゴリズムの実現です。計算スピードが遅くためラブライブを用いて要約をやってみます\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0S6TNPktOj-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "5717018d-26a7-4309-d6db-d7daf7cd34e4"
      },
      "source": [
        "model_path = '/content/word2vec.gensim.model'\n",
        "m = Word2Vec.load(model_path)\n",
        "#wv = KeyedVectors.load_word2vec_format(model_path, binary=True,encoding=\"utf-8\")\n",
        "word_embeddings = {}\n",
        "print(m[\"新型コロナウイルス\"].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(50,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oupywzhoDpQs"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY7UUZHH9P-F"
      },
      "source": [
        "#sentence類似度計算\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model_path = '/content/word2vec.gensim.model'\n",
        "m = Word2Vec.load(model_path)\n",
        "#wv = KeyedVectors.load_word2vec_format(model_path, binary=True,encoding=\"utf-8\")\n",
        "word_embeddings = {}\n",
        "\n",
        "mt = MeCab.Tagger(\"-Owakati\")\n",
        "#print(m[\"富士通\"])\n",
        "#node = mt.parse(\"昨日、お笑い番組を見た。\")\n",
        "\n",
        "\n",
        "def get_vector(text):\n",
        "  sum_vec =np.zeros(50)\n",
        "  word_count = 0\n",
        "  node = mt.parse(text)\n",
        "  #print(node)\n",
        "  for word in node.split():\n",
        "    print(word)\n",
        "    sum_vec +=m[word]\n",
        "    word_count += 1\n",
        "    #word = word.next\n",
        "  return sum_vec/word_count\n",
        "\n",
        "\"\"\"\n",
        "  while node:\n",
        "    print(node+\"/n\")\n",
        "    #fields = node.feature.split(\",\")\n",
        "    sum_vec +=m[node]\n",
        "    word_count += 1\n",
        "    node = node.next\n",
        "  return sum_vec/word_count\n",
        "\"\"\"\n",
        "\n",
        "#print(get_vector(\"富士通と東京品川病院は9月2日、新型コロナウイルス肺炎の診断に有効とされる胸部CT(Computed Tomography:コンピューター断層撮影)検査による画像診断の支援を行うAI技術の共同研究開発を同日から実施すると明らかにした。新型コロナウイルス感染の疑いが強い患者の治療を行う際、PCR検査結果だけでなく、血液検査や胸部CT検査など、そのほかの検査結果を踏まえて総合的に診断および治療方針の決定を行う。PCR検査が陰性でも、そのほかの検査で新型コロナウイルス肺炎と診断される場合もあり、胸部CT検査による画像診断は重要な位置づけとなっているという。肺疾患の診断では、医師が患者の胸部CT画像に映った病変部の陰影の特徴に基づき診断しているが、異常な陰影の判別だけでなく、肺全体にわたる陰影の立体的な分布状況を把握するため、患者一人あたり数百枚にもおよぶ胸部CT画像を目視で確認している。特に、新型コロナウイルス感染症拡大に伴い、医師の負担を軽減し、スピーディーな診断を支援する技術が求められているほか、問診で新型コロナウイルス感染の可能性が低く、PCR検査が実施されない場合、その患者の胸部CT画像から新型コロナウイルス肺炎を見つけ出すことも重要だという。これらの診断支援の必要性から両者は共同で、新型コロナウイルス肺炎の診断に有効な手法とされる胸部CT検査に対して、AIを活用した医師の画像診断支援技術の開発を開始。具体的には、東京品川病院が有する過去の新型コロナウイルス肺炎の胸部CT画像データから肺の異常陰影パターンを検出して、それらのデータをAIに学習させることで新型コロナウイルス肺炎の可能性を示すAI技術を開発し、その技術の有効性を両者で検証する。医師が新型コロナウイルス肺炎を診断する際、肺に見られる異常陰影のパターンと肺全体の異常陰影の広がり方が診断のための重要な情報となっており、異常陰影のパターンの検出は富士通研究所が開発したAIを活用して行い、CT画像上で肺を右肺末梢、右肺中枢、左肺中枢、左肺末梢の4つの領域に分割し、各領域の上下方向の陰影分布をヒストグラム化(データの分布状況を視覚的に認識可能な統計グラフの1つ)する。これにより、三次元的な陰影の広がりを数値化し、検出された異常陰影パターンと陰影分布を用いて新型コロナウイルス肺炎を判別するAIを新たに開発。AIで新型コロナウイルス肺炎の可能性を示すことで、医師が\"))\n",
        "def cos_sim(v1, v2):\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    v1 = get_vector('昨日、お笑い番組を見た。')\n",
        "    v2 = get_vector('昨夜、テレビで漫才をやっていた。')\n",
        "    v3 = get_vector('昨日、公園に行った。')\n",
        "\n",
        "    print(cos_sim(v1, v2))\n",
        "    print(cos_sim(v1, v3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWAV6vp2wKXC"
      },
      "source": [
        "本作業"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf0-WBOZhMm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c0a415-0043-405a-a286-ca61a7779e2f"
      },
      "source": [
        "a = open(\"/content/test.txt\",\"r\")\n",
        "sentence_word_list = []\n",
        "for sentence in a.readlines():\n",
        "    line_seg = sentence\n",
        "    sentence_word_list.append(line_seg)\n",
        "print(\"一共有\",len(sentence_word_list),'个句子。\\n')\n",
        "print(sentence_word_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "一共有 13 个句子。\n",
            "\n",
            "['富士通と東京品川病院は9月2日、新型コロナウイルス肺炎の診断に有効とされる胸部CT(sコンピューター断層撮影)検査による画像診断の支援を行うAI技術の共同研究開発を同日から実施すると明らかにした。\\n', '新型コロナウイルス感染の疑いが強い患者の治療を行う際、PCR検査結果だけでなく、血液検査や胸部CT検査など、そのほかの検査結果を踏まえて総合的に診断および治療方針の決定を行う。\\n', 'PCR検査が陰性でも、そのほかの検査で新型コロナウイルス肺炎と診断される場合もあり、胸部CT検査による画像診断は重要な位置づけとなっているという。\\n', '肺疾患の診断では、医師が患者の胸部CT画像に映った病変部の陰影の特徴に基づき診断しているが、異常な陰影の判別だけでなく、肺全体にわたる陰影の立体的な分布状況を把握するため、患者一人あたり百枚にもおよぶ胸部CT画像を目視で確認している。\\n', '特に、新型コロナウイルス感染症拡大に伴い、医師の負担を軽減し、スピーディーな診断を支援する技術が求められているほか、問診で新型コロナウイルス感染の可能性が低く、PCR検査が実施されない場合、その患者の胸部CT画像から新型コロナウイルス肺炎を見つけ出すことも重要だという。\\n', 'これらの診断支援の必要性から両者は共同で、新型コロナウイルス肺炎の診断に有効な手法とされる胸部CT検査に対して、AIを活用した医師の画像診断支援技術の開発を開始。\\n', '具体的には、東京品川病院が有する過去の新型コロナウイルス肺炎の胸部CT画像データから肺の異常陰影パターンを検出して、それらのデータをAIに学習させることで新型コロナウイルス肺炎の可能性を示すAI技術を開発し、その技術の有効性を両者で検証する。\\n', '医師が新型コロナウイルス肺炎を診断する際、肺に見られる異常陰影のパターンと肺全体の異常陰影の広がり方が診断のための重要な情報となっており、異常陰影のパターンの検出は富士通研究所が開発したAIを活用して行い、CT画像上で肺を肺末梢、肺中枢、肺中枢、肺末梢の4つの領域に分割し、各領域の上下方向の陰影分布をヒストグラム化(データの分布状況を視覚的に認識可能な統計グラフの1つ)する。\\n', 'これにより、三次元的な陰影の広がりを数値化し、検出された異常陰影パターンと陰影分布を用いて新型コロナウイルス肺炎を判別するAIを新たに開発。\\n', 'AIで新型コロナウイルス肺炎の可能性を示すことで、医師が胸部CT画像から肺炎の診断をする際、陰影の立体的な広がりを百枚の胸部CT画像から目視で確認していた診断時間を短縮し、専門医に加え、新型コロナウイルス肺炎の診断の効率化を図る。\\n', '今後、両者は共同研究開発を通じて、新型コロナウイルス肺炎の診断に活用されるさまざまな情報を利用できる技術を確立することで、AIによる胸部CT画像診断から新型コロナウイルス肺炎の画像診断支援技術の向上を目指す考えだ。\\n', '富士通は、ヘルスケアソリューションとして同技術のサービス化を検討することに加え、電子カルテ情報とも連携させることで、胸部CT画像をもとにした医師の診断支援だけでなく、同技術の活用領域の拡大を目指す。\\n', '一方、東京品川病院は院内で実施しているさまざまな研究との融合を目指し、新型コロナウイルス肺炎の診断治療に役立てるという。']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjKZ3T1ix6yk"
      },
      "source": [
        "#句子向量\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import MeCab\n",
        "\n",
        "model_path = '/content/word2vec.gensim.model'\n",
        "m = Word2Vec.load(model_path)\n",
        "#wv = KeyedVectors.load_word2vec_format(model_path, binary=True,encoding=\"utf-8\")\n",
        "word_embeddings = {}\n",
        "\n",
        "mt = MeCab.Tagger(\"-Owakati\")\n",
        "#print(m[\"富士通\"])\n",
        "#node = mt.parse(\"昨日、お笑い番組を見た。\")\n",
        "\n",
        "\n",
        "def get_vector(text):\n",
        "  sum_vec =np.zeros(50)\n",
        "  word_count = 0\n",
        "  node = mt.parse(text)\n",
        "  #print(node)\n",
        "  for word in node.split():\n",
        "    #print(word)\n",
        "    sum_vec +=m[word]\n",
        "    word_count += 1\n",
        "    #word = word.next\n",
        "    #print(sum_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNPr-GEJ1Uxf"
      },
      "source": [
        "問題点：sentence毎にvector化しているので、ドキュメントはダメ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRvhpbb4PeAJ"
      },
      "source": [
        "#句子矩阵\n",
        "\n",
        "#print(len(a.readlines()))\n",
        "sentence_vectors = []\n",
        "for i in a.readlines():\n",
        "  print(get_vector(i))\n",
        "  sentence_vectors.append(get_vector(i))\n",
        "#print(sentence_vectors)\n",
        "#sentence_vectors -->list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lby-ql-dfXiJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "4bdf2674-f8eb-4d34-edda-9b53b367a14a"
      },
      "source": [
        "#句子相似矩阵\n",
        "import numpy as np\n",
        "\n",
        "sim_mat = np.zeros([13,13])\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "for i in range(13):\n",
        "  for j in range(13):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,50), sentence_vectors[j].reshape(1,50))[0,0]\n",
        "print(\"句子相似度矩阵的形状为：\",sim_mat.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-73c42d6613c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0msim_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"句子相似度矩阵的形状为：\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msim_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvQn0qMFf7Ti",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "268bf20d-fbb0-47d5-c7ec-68c31093287d"
      },
      "source": [
        "\"\"\"第五步：迭代得到句子的textrank值，排序并取出摘要\"\"\"\n",
        "import networkx as nx\n",
        "\n",
        "# 利用句子相似度矩阵构建图结构，句子为节点，句子相似度为转移概率\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "\n",
        "# 得到所有句子的textrank值\n",
        "scores = nx.pagerank(nx_graph)\n",
        "\n",
        "# 根据textrank值对未处理的句子进行排序\n",
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentence_word_list)), reverse=True)\n",
        "\n",
        "# 取出得分最高的前10个句子作为摘要\n",
        "sn = 3\n",
        "for i in range(sn):\n",
        "    print(\"第\"+str(i+1)+\"条摘要：\\n\\n\",ranked_sentences[i][1],'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "第1条摘要：\n",
            "\n",
            " 具体的には、東京品川病院が有する過去の新型コロナウイルス肺炎の胸部CT画像データから肺の異常陰影パターンを検出して、それらのデータをAIに学習させることで新型コロナウイルス肺炎の可能性を示すAI技術を開発し、その技術の有効性を両者で検証する。\n",
            " \n",
            "\n",
            "第2条摘要：\n",
            "\n",
            " AIで新型コロナウイルス肺炎の可能性を示すことで、医師が胸部CT画像から肺炎の診断をする際、陰影の立体的な広がりを百枚の胸部CT画像から目視で確認していた診断時間を短縮し、専門医に加え、新型コロナウイルス肺炎の診断の効率化を図る。\n",
            " \n",
            "\n",
            "第3条摘要：\n",
            "\n",
            " これらの診断支援の必要性から両者は共同で、新型コロナウイルス肺炎の診断に有効な手法とされる胸部CT検査に対して、AIを活用した医師の画像診断支援技術の開発を開始。\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wus_dlei1lgm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7ffaef7e-4278-46b8-c403-d5d813063355"
      },
      "source": [
        "#分割句子 未实现\n",
        "a = open(\"/content/news.txt\",\"r\")\n",
        "#print(a.readline(0))\n",
        "b = open(\"/content/renews.txt\",\"w\")\n",
        "for line in a.readlines(0):\n",
        "  print(line.split(\"。\"))\n",
        "  b.write(str(line.split(\"。\")))\n",
        "  b.write(\"\\n\")\n",
        "b.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['富士通と東京品川病院は9月2日、新型コロナウイルス肺炎の診断に有効とされる胸部CT(Computed Tomography:コンピューター断層撮影)検査による画像診断の支援を行うAI技術の共同研究開発を同日から実施すると明らかにした', '新型コロナウイルス感染の疑いが強い患者の治療を行う際、PCR検査結果だけでなく、血液検査や胸部CT検査など、そのほかの検査結果を踏まえて総合的に診断および治療方針の決定を行う', 'PCR検査が陰性でも、そのほかの検査で新型コロナウイルス肺炎と診断される場合もあり、胸部CT検査による画像診断は重要な位置づけとなっているという', '肺疾患の診断では、医師が患者の胸部CT画像に映った病変部の陰影の特徴に基づき診断しているが、異常な陰影の判別だけでなく、肺全体にわたる陰影の立体的な分布状況を把握するため、患者一人あたり数百枚にもおよぶ胸部CT画像を目視で確認している', '特に、新型コロナウイルス感染症拡大に伴い、医師の負担を軽減し、スピーディーな診断を支援する技術が求められているほか、問診で新型コロナウイルス感染の可能性が低く、PCR検査が実施されない場合、その患者の胸部CT画像から新型コロナウイルス肺炎を見つけ出すことも重要だという', 'これらの診断支援の必要性から両者は共同で、新型コロナウイルス肺炎の診断に有効な手法とされる胸部CT検査に対して、AIを活用した医師の画像診断支援技術の開発を開始', '具体的には、東京品川病院が有する過去の新型コロナウイルス肺炎の胸部CT画像データから肺の異常陰影パターンを検出して、それらのデータをAIに学習させることで新型コロナウイルス肺炎の可能性を示すAI技術を開発し、その技術の有効性を両者で検証する', '医師が新型コロナウイルス肺炎を診断する際、肺に見られる異常陰影のパターンと肺全体の異常陰影の広がり方が診断のための重要な情報となっており、異常陰影のパターンの検出は富士通研究所が開発したAIを活用して行い、CT画像上で肺を右肺末梢、右肺中枢、左肺中枢、左肺末梢の4つの領域に分割し、各領域の上下方向の陰影分布をヒストグラム化(データの分布状況を視覚的に認識可能な統計グラフの1つ)する', 'これにより、三次元的な陰影の広がりを数値化し、検出された異常陰影パターンと陰影分布を用いて新型コロナウイルス肺炎を判別するAIを新たに開発', 'AIで新型コロナウイルス肺炎の可能性を示すことで、医師が胸部CT画像から肺炎の診断をする際、陰影の立体的な広がりを数百枚の胸部CT画像から目視で確認していた診断時間を短縮し、専門医に加え、新型コロナウイルス肺炎の診断の効率化を図る', '今後、両者は共同研究開発を通じて、新型コロナウイルス肺炎の診断に活用されるさまざまな情報を利用できる技術を確立することで、AIによる胸部CT画像診断から新型コロナウイルス肺炎の画像診断支援技術の向上を目指す考えだ', '富士通は、ヘルスケアソリューションとして同技術のサービス化を検討することに加え、電子カルテ情報とも連携させることで、胸部CT画像をもとにした医師の診断支援だけでなく、同技術の活用領域の拡大を目指す', '一方、東京品川病院は院内で実施しているさまざまな研究との融合を目指し、新型コロナウイルス肺炎の診断治療に役立てるという', '\\n']\n",
            "['2020年の主要16品目の半導体市場規模は、新型コロナウイルス感染症の影響による外出制限に伴うリモートワークの推進などを追い風に、半導体デバイスの需要が増加した結果、前年比14.4％増の26兆678億円が見込まれるという', 'その後も米中貿易摩擦や日韓貿易摩擦による半導体市場への影響が見受けられるものの、リモートワークの一般化、AIの普及などに伴い、半導体の活用が進むとみられ、2025年にはその市場規模は2019年比88.9％増の43兆470億円に到達するものと同社は予測している', '品目別にみると、DRAMの市場規模がもっとも大きく、次いでNAND、モバイル機器用APと続く', 'DRAMやNANDは、データセンターの投資が好調であることから継続的に伸長していくとみられる', 'スマートフォン(スマホ)やスマートグラスなどに搭載されるモバイル機器用APは、5G通信のサービス開始に伴い5G対応スマートフォンの需要が高まっていることや、スマートウォッチやスマートグラスなどへ用途が広がっていることから、2021年以降市場は堅調に拡大していくと予想されるという', 'もっとも大きな市場として期待されるDRAMの2020年市場について同社は、前年比19.4％増の8兆円と見込んでいる', 'CPUの開発遅延などにより投資が緩やかであったデータセンターにおいて、リモートワークの普及によるデータ通信料の増加や代替CPUの登場によって投資が活発化しており、今後も投資が継続していくことが期待されるため、2025年には2019年比94.0％増の13兆円に達すると予測している', 'メモリ分野のもう1つの大市場であるNANDの2020年市場規模は前年比34.9％増の6兆8000億円が見込まれると富士キメラでは見込んでいる', 'また、今後もデータセンターを中心にストレージに対する投資が進むことから、需要は堅調に推移し、2025年には2019年比で3.1倍となる15兆5000億円に達すると予測している', '半導体業界が期待してきた自動車分野の中核の1つ、自動車用SoCの2020年市場は、新型コロナの影響で自動車販売が不振に陥っていることから前年比12.2％減の2030億円となると見込まれているが、今後は自動運転レベルの向上に伴い、1台当たりの搭載個数は増加していくこととなり、需要の増加が期待されることから、2025年には2019年比2.0倍の4659億円に達すると予測している', 'スマートフォンへの搭載数拡大のほか、自動車での活用も進むイメージセンサ市場について同社は、2020年が前年比3.0％増の1兆9680億円と見込んでいる', '現在、スマートフォンへの搭載数が増加しているが、今後もそのトレンドは継続するほか、自動車でも搭載数が増加していくと予測されるため、市場の拡大が続くことが期待され、2025年には2019年比38.5％増の2兆6460億円に達すると予測している', 'そしてセンサ系としては、ToFセンサについて同社は、5Gや画像認識技術の進化に伴い、ARの普及が進むことが期待されており、2020年の市場規模は前年比6.4％増の1055億円と見込んでいる', 'また、スマホへの搭載は当然ながら、中長期的にはスマートグラスへの搭載も進むことが期待されているため、2025年には2019年比3.2倍増の3143億円と予測している', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faMDWr_PikK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "a26670c2-99a1-47cf-a329-6f5d76425296"
      },
      "source": [
        "##to-do 把上面的功能进行汇总\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model_path = '/content/word2vec.gensim.model'\n",
        "m = Word2Vec.load(model_path)\n",
        "#wv = KeyedVectors.load_word2vec_format(model_path, binary=True,encoding=\"utf-8\")\n",
        "word_embeddings = {}\n",
        "\n",
        "mt = MeCab.Tagger(\"-Owakati\")\n",
        "\n",
        "a = open(\"/content/test.txt\",\"r\")\n",
        "for i in a.readlines():\n",
        "  print(get_vector(i))\n",
        "\n",
        "#sim_mat = np.zeros([len(sentences_list), len(sentences_list)])\n",
        "#類似度の計算はこち使うかな？\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sim_mat = np.zeros([len(sentences_list), len(sentences_list)])\n",
        "a = open(\"/content/test.txt\",\"r\")\n",
        "for i in range(len(13)):\n",
        "  for j in range(len(13)):\n",
        "    sim_mat[i][j] = cosine_similarity(get_vector[i].reshape(1,50), get_vector[j].reshape(1,50))[0,0] \n",
        "\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)\n",
        "\n",
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "# Specify number of sentences to form the summary\n",
        "sn = 10\n",
        "\n",
        "# Generate summary\n",
        "for i in range(sn):\n",
        "  print(ranked_sentences[i][1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-c6697d085bc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msim_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#類似度の計算はこち使うかな？\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sentences_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QEmGv-3ikDw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98ca88fb-c904-4748-c8e9-a5fc3d8d6f95"
      },
      "source": [
        "a = np.array([[0.25,0.3,0.363,0.512],[0.12,0.25,0.33,0.15],[1.2,2.3,0.36,0.44]])\n",
        "a\n",
        "print(a.mean(axis=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.35625 0.2125  1.075  ]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}